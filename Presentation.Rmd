---
title: "Credit Sesame Data Analysis and Findings"
author: "Hari Rajan, Paul Giraud, Audreya Metz, and Calvin Ma"
date: September 27 2018
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
fig_width: 4
fig_height: 2
          

font-size: 12px
---
```{r load, warning=FALSE, include=FALSE}
#Load necessary packages
options(warn=-1)
suppressWarnings(library("magrittr"))
suppressWarnings(library("tidyr"))
suppressWarnings(library("ggplot2"))
suppressWarnings(library("readr"))
```

```{r 1, warning=FALSE, cache=TRUE, include=FALSE}
#Removing columns that are redundant and cover the same information
user_data = read.csv('user_profile.csv')

#Removing columns that are redundant and cover the same information
data.pre = user_data[-c(1, 3, 4, 5, 27, 28, 36)]

#ishomeowner, gender(remove unisex), remove NA from 8, 9, 10, 27, 28
data.pre = subset(data.pre, (data.pre$gender == 'Male' | data.pre$gender == 'Female'))
data.pre= na.omit(data.pre)
data.pre$gender = data.pre$gender == 'Female'
data.pre$is_homeowner = data.pre$is_homeowner == 'True'
cols <- sapply(data.pre, is.logical)
data.pre[,cols] <- lapply(data.pre[,cols], as.numeric)

#turning the buckets into continous values - we average the bucket's min & max to the middle value

#cor(data.pre[-1])

decode_bucket <- function(fctr) {
  fctr <- gsub("[()]", '', fctr)
  fctr <- gsub("\\]", '', fctr)
  bounds <- strsplit(fctr, ", ")
  return(mean(as.numeric(bounds[[1]])))
}
data.pre$age = unlist(lapply(data.pre$age_bucket, FUN = decode_bucket))
data.pre$credit_score = unlist(lapply(data.pre$credit_score_bucket, FUN = decode_bucket))
data.pre = na.omit(data.pre)

#removing bucketed columns
data.pre = data.pre[-c(31,32)]

#reading in user engagement data
user_engage_data = read.csv('user_engagement.csv')

#We are taking the click counts for differnt types of pages
user_engage_actions = user_engage_data[c(21, 22, 23, 24, 25, 26)]
```

Dataset
===

- The data was from Credit Sesame, a company that calculates credit scores to determine options for credit cards, mortgage rates and loans. 
- This dataset included: 
      - User Demographics
      - First Session Information
      - 30-day User Engagement Data

Our Research Direction
===

- We wanted to give Credit Sesame an insightful description of their user base
- We believe that they would have an interest in the credit score of their users, since this helps the to predict the best loans, mortgages, etc.

Exploring the Dataset
===
- We started out EDA by looking at the distribution of the variables in the datset
- We found a significant right skew in the majority of the non-categorical data
```{r 3, echo=FALSE, cache=TRUE, warning=FALSE, fig.width=9, fig.height=3}
ggplot(gather(data.pre[c(2,5,13)]), aes(value)) + 
    geom_histogram(bins=10) + 
    facet_wrap(~key, scales = 'free_x')
```
This was important to keep in mind as we proceeded in our analysis as the distribution of each variable can impact the results of our analysis methods.

Exploring the Dataset
===

We also wanted to see if there was any obvious relationship between credit score and the others features in the user demographics. 

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.width=9, fig.height=3}
par(mfrow=c(1,3))
plot(x = data.pre$total_tradelines_amount_past_due, y = data.pre$credit_score, xlab = "total_tradelines_amount_past_due",
     ylab = "credit_score") #yes
plot(x = data.pre$max_cc_limit, y = data.pre$credit_score, xlim = c(0, 150000), xlab = "max_cc_limit",
     ylab = "credit_score") #yes
plot(x = data.pre$count_bankruptcy, y = data.pre$credit_score, xlab = "count_bankruptcy",
     ylab = "credit_score") #yes
```
Several of the features had a clear linear relationship with credit score, which motivated a linear regression to determine which features had the greatest impact on credit score. Other features didnt have such an obvious relationship.


Exploring the Dataset
===
We ended our EDA using PCA to determine any relationships between data points. Due to the high-dimensionality, the biplot is hard to interpret, but we can still obtain some valuable information from the plot. 
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.height=6}
data.pre.pca = data.pre[-c(1, 5, 6, 9, 12, 13, 14, 19, 20, 23, 27, 29)]


data.pca = prcomp(data.pre.pca[c(1:90000),], scale=TRUE)
biplot(data.pca, scale=0, cex=0.7)


```

Challenges of the Dataset
===

- The skewed distribution of our variables was the biggest challenge for determining how to analyze this data.
- Ultimately we did not let the skew effect our methods but were careful to consider it in our analysis.

Methods
===

- Linear Regression 
    - We determined which features to use in the regression using the plots and PCA
    - Only achieved an R-squared of .35, which is too low to draw meanginful conclusions
    - COuld use other regression methods such as Lasso in the future
- K-means clustering
    - Cluster users based on most frequent page type
    - Compare within cluster user demographics with those of other clusters
    - Find differences between users

K-means
===
- We use k-means because it is the quicker than hierarchical clustering, but we first need to define k. We look at the plot of k vs. WSS. 

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.height=6}

k.max <- 10
data <- user_engage_actions
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k, nstart=5,iter.max = 10 )$tot.withinss})

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")


```
We use the Elbow method to determine the optimum number of k. There is clearly an elbow at k=6 where the WSS decreases at a much slower rate after we get to 6 and it decreasing relatively consistantly up until 6. We set k=6. 

Results
===

- After determining the number of clusters to use and accordingly performing the k-means clustering method, we noticed that in particular, there were two clusters which had relatively high values for `click_count_credit_card` and `click_count_personal_loan.`

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.height=6, size = 'tiny'}
set.seed(1)
kmeanclusters = kmeans(user_engage_actions, 6, nstart=20)
kmeanclusters$centers[,c(1,2)]
```


Results (continued)
===

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.height=6}

click_credit_card = user_engage_data[which(kmeanclusters$cluster == 3),]

click_personal_loan = user_engage_data[which(kmeanclusters$cluster == 6),]


#Removing columns that are redundant and cover the same information


click_credit_users = subset(data.pre, data.pre$user_id %in% click_credit_card$user_id)
click_loan_users = subset(data.pre, data.pre$user_id %in% click_personal_loan$user_id)

click_credit_users$is_credit = rep(TRUE, nrow(click_credit_users))
click_loan_users$is_credit = rep(FALSE, nrow(click_loan_users))


allusers = rbind(click_credit_users, click_loan_users)


allusers= na.omit(allusers)
allusers[,cols] <- lapply(allusers[,cols], as.numeric)


#turning the buckets into continous values - we average the bucket's min & max to the middle value


decode_bucket <- function(fctr) {
  fctr <- gsub("[()]", '', fctr)
  fctr <- gsub("\\]", '', fctr)
  bounds <- strsplit(fctr, ", ")
  return(mean(as.numeric(bounds[[1]])))
}
allusers$age = unlist(lapply(allusers$age_bucket, FUN = decode_bucket))
allusers$credit_score = unlist(lapply(allusers$credit_score_bucket, FUN = decode_bucket))
allusers = na.omit(allusers)


#removing bucketed columns



#data.pre is our full dataset 

allusersdata = allusers[-c(1, 5, 6, 9, 12, 13, 14, 19, 20, 23, 27, 29, 31)]


allusers.pca = prcomp(t(allusersdata), scale=TRUE, retx = TRUE)

x = allusers.pca$rotation[,1]
y = allusers.pca$rotation[,2]

plot(x, y, col = ifelse(allusers$is_credit == TRUE, 'blue', 'red'), xlab = 'PC1', ylab = 'PC2')

```

Results (continued)
===

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.height=6}
options(scipen=999)

compare = matrix(c(mean(click_credit_users$count_tradelines_open_collection_accounts), mean(click_loan_users$count_tradelines_open_collection_accounts),
                   mean(click_credit_users$total_tradelines_amount_past_due), mean(click_loan_users$total_tradelines_amount_past_due),
                   mean(click_credit_users$total_cc_open_balance), mean(click_loan_users$total_cc_open_balance),
                   mean(click_credit_users$max_cc_limit), mean(click_loan_users$max_cc_limit),
                   mean(click_credit_users$total_student_loans_balance), mean(click_loan_users$total_student_loans_balance),
                   mean(click_credit_users$credit_score), mean(click_loan_users$credit_score)),ncol = 2, byrow = TRUE)


colnames(compare) = c("click_credit", "click_loan")
rownames(compare) = names(click_credit_users[c(11, 17, 19, 21, 25, 32)])
compare
```


Conclusions
===

- The k-means clustering allowed us to confirm that the demographics of people who use Credit Sesame's website differently and access different pages are in fact different 
- There is a distinction between Credit Sesame's website clientele that uses the credit card features and those who use loan features
    - The credit page users tend to have lower credit scores and the loan feature users tend to have higher loan amounts based on grouped findings
    -	The largest difference between credit score users and loan feature users are open collection accounts, # of derogatory tradelines, max credit card limit, total credit card balance, and total mortgage loans and balance, which all happen to be strong indicators of credit score
    
Conclusions
===

-	We can work to improve the current models that we used
    - Regression model can be improved using methods such as Lasso or ElasticNet to facilitate feature selection 
    -	Hierarchal clustering could have provided more insight into potential variable groups
