---
title: "Credit Sesame Consumer Report"
author: "Audreya Metz, Calvin Ma, Hari Rajan, Paul Giroud"
keywords: pandoc, r markdown, knitr
output: pdf_document

abstract: ""
---

# 1 Introduction

Determining the appropriate consumer to give loans to is a difficult task that can depend on many factors. Some of which directly impact what a company may think of you such as the amount past due on accounts, your open balance, etc. Other factors are more implicit, but could also determine whether or not you'd be a good candidate to give a loan to. We analyzed data from Credit Sesame, a company that calculate credit scores to determine options for credit cards, mortgage rates and loans. This dataset included three main parts: User Demographics, First Session Information and 30-day User Engagement Data. The user demographics data included mostly credit profile information including loan histories, tradeline details, etc. There were also a few personal features such as gender and location. The First Session data provided action logs of each user's first interaction with the Credit Sesame website. Finally, the 30-day user engagement gave insight into the actions of users in each one's first 30 days. This dataset had similar features to the first session, but was sparser and included various logins for each user. 

We believe that identifying accurate credit scores with both financial and non-financial data can be of use to Credit Sesame. If Credit Sesame could approximate credit scores from certain characteristics, they could then provide targeted advertisements for specific credit cards or loans. In addition to targeted offerings and ads, using more statistically significant data when calculating credit score could lead more better predictions of user's credit, which could lead to Credit Sesame making better decision on who to give loans to based on user's credit score. Our ultimate goal was to see what cariables were strong indicators of credit score and delinquencies. We also found it relevant to explore the engagement habits of certain demographics groups with the Credit Sesame website. By analyzing how certain groups differ in website navigation, we could provide insight to Credit Sesame on their customer base. Credit Sesame could use this to develop internal strategies and how to better retain users based on their needs and profiles.

To accomplish these tasks, we used 4 tools: PCA, Linear Regression, Lasso Regression and K-means clustering. PCA, linear regression and lasso regression were used with the user demographics data to explore which features had a significant impact on the credit score, and by how much. The k-mean clustering was used with both the user demographics and 30-day engagement to cluster similar users based on their engagement and then compare the difference between users of differing clusters. 

ADD IN SUMMARY OF RESULTS

# 2 Data set

## 2.1 Exploratory Data Analysis
HARI, PAUL, CALVIN ADD IN EDA. (We should mention that some variables are very correlated - the correlation matrix is easy to make)

## 2.2 Preprocessing 

We did different forms of preprocessing for each of the different methods (from working separately on different parts). 

```{r, echo=FALSE}
#Reading in the data and preprocessing it
user_data = read.csv('user_profile.csv')
summary(user_data)

#Removing columns that are redundant and cover the same information
data.pre = user_data[-c(1, 3, 4, 5, 27, 28, 36)]

#ishomeowner, gender(remove unisex), remove NA from 8, 9, 10, 27, 28
data.pre = subset(data.pre, (data.pre$gender == 'Male' | data.pre$gender == 'Female'))
data.pre= na.omit(data.pre)
data.pre$gender = data.pre$gender == 'Female'
data.pre$is_homeowner = data.pre$is_homeowner == 'True'
cols <- sapply(data.pre, is.logical)
data.pre[,cols] <- lapply(data.pre[,cols], as.numeric)

#turning the buckets into continous values - we average the bucket's min & max to the middle value
decode_bucket <- function(fctr) {
  fctr <- gsub("[()]", '', fctr)
  fctr <- gsub("\\]", '', fctr)
  bounds <- strsplit(fctr, ", ")
  return(mean(as.numeric(bounds[[1]])))
}
data.pre$age = unlist(lapply(data.pre$age_bucket, FUN = decode_bucket))
data.pre$credit_score = unlist(lapply(data.pre$credit_score_bucket, FUN = decode_bucket))
data.pre = na.omit(data.pre)

#removing bucketed columns
data.pre = data.pre[-c(31,32)]
```

```{r}
data.pre.pca = data.pre[-c(1, 5, 6, 9, 12, 13, 14, 19, 20, 23, 27, 29)]
```

* 2.2.1 PCA: To clean the user profile data and make it more manageable, we removed any rows with any NA's and removed users with gender marked as "unisex". We removed the unisex because there was an extremely large number of unisex users, which led us to believe that unisex also represented the users who didn't want to choose which sex they were. This cut down on our sample sizebut we still had quite a lot of data. We also removed the user_signup_timesteamp, state and zipcode features since we decided they were unnecessary to the PCA. For other features, there seemed to be overlapping information that were redundant and colinear- Cases like avg_days vs. max_days were not both necessary since avg_days accounts for max_days, so we removed rows that represented the same information. Also, cases where information could be represented by total tradeline data, but also had information split between banking, credit, etc. also presented the same information. While these could individually important, we only wanted general credit information for the PCA, so we removed any columns that represented a split of tradeline information (if tradeline info was unavailable, we kept the subset features of tradeline). Finally, in dealing with bucketed age and credit scores, we parsed the min and ma of the buckets and had numerical values as the average of the min & max of the bucket. This means that we don't have to deal with numberous dummy variables. 


* 2.2.2 Regression

* 2.2.4 Lasso: We took a similar process of preprocessing lasso - we removed the NAs and unisex users, and removed unnecessary features (user_signup_timestamp, state and zipcode). We also parsed the bucketed features like in PCA. However, we decided not to remove redundent, colinear values because lasso naturally does this during the regression. 

```{r}
#reading in data
user_engage_data = read.csv('user_engagement.csv')

summary(user_engage_data)

#We are taking the click counts for differnt types of pages
user_engage_actions = user_engage_data[c(21, 22, 23, 24, 25, 26)]
```

* 2.2.3 $k$-Means: The k-means clustering was meant as a way to cluster users based on their engagement. We decided that the best way to gauge engagement is based on the number of clicks per page. So, we took the 30 day engagement dataset and only took the features that represented clicks per various pages like credit cards, loans, etc. Although the dataset was very sparse, we didn't need to preprocess it any further. 


# 3 Methods

* 3.1 PCA: We used Principle Component Analysis because it is a powerful tool that can be used for both exploratory data analysis and also making conclusions about the dataset. PCA continually takes the axis of greatest variance and calculates the correlation coefficients along this axis as loadings. We wanted some method to visualize the relationship between data points, but the dataset's dimensionality was too large to plot. Using PCA, we could plot two or three of the Principle Components in order to have some sort of visualization of the data, and can offer some intuition of the relationships between data. In plotting it, we were hoping to see two or more distinct clusters of datapoints, but we were left with a large clump. However, the biplot and loadings revealed interesting information that led to other discoveries. Interpreting the PCA is difficult, but by analyzing the loadings, we can see how correlated various features are within each principle component, which tells us which features are related to each other. 

* 3.2.1 Linear Regression

* 3.2.2 Lasso: Regular linear regression cannot work properly when features are highly correlated. However, Lasso is a form of linear regression that allows us to input a dataset with correlated variables. Lasso will account for these variables and remove them based on the lambda regularization. We used this method because our process of removing features by hand using the R-squared was slow and had a lot of room of error. We also had to do a lot of preprocessing for linear regression that could have removed potentially important features. Using lasso, we were free to input the entire dataset without worrying about correlated variables. Overally, we implemented lasso as an alternative to linear regression that would work better with the dataset due to the correlated variables. 

* 3.3 $k$-Means: K-means clustering allows us to find groups of datapoints together based on their distances from each other. By dictating a number of clusters we expect to see, the algorithm will find the closest subset of datapoints for each of clusters. This model is useful because it gives us a definitive way to find which datapoints are related based on a variety of features. For our dataset, we can take datapoints for different clusters and view them as a collective, and find the demographics of the clusters based on its datapoints. 

# 4 Applications

```{r}
#doing PCA over the preprocessed dataset and saving loadings 
data.pca = prcomp(data.pre.pca, scale=TRUE)
loadings = data.pca$rotation
```

* 4.1 Applications for PCA: After doing PCA on the user_demographics, the first thing that we looked at was the plot of PC1 vs. PC2. 
```{r, echo=FALSE}
plot(data.pca$x[,1:2])
```

Initially, we were hoping that we could find distinct clusters of users based on their traits. However, the plot is clearly just a large clump of datapoints. To find some meaning from it, we also plotted the biplot of the PCA. 

```{r, echo=FALSE}
biplot(data.pca, scale=0, cex=0.7)
```
The biplot reveals much more information about the loadings and relationships between values. We can see that the entire first loading is negative. The second loading is much more interesting - there is a split between the PC2 loadings. Either the value is relatively positive or negative, there are only a couple features with a moderate loadings. It is difficult to interpret the meaning of the loadings as a whole, but we can see that the positive correlations easily differentiable from the negative loadings. Next, we looked at the loadings themselves and scree plot of the PCs. 
```{r, echo=FALSE}
screeplot(data.pca, type = "lines", main = "Principle Components vs. Variance")
```
From the scree plot, we can see that there are large differences in variance from PCs 1 to 4. Once we get to PC4, we see a small change in the decrease of variation, which means PC4 is the last significant PC that provides a large decrease in variance. Finally, we interpreted the Principle Components loadings by looking at PC1 through PC4. 
```{r, echo=FALSE}
loadings[,1:4]
```
The values of the loadings correspond to the correlation coefficient of the variable with that principle component. What we are looking for in the loadings are values that are related to credit_score. From the first PC, we see that all of the correlation values are negative, some more than others. Credit_score has a loading of -0.23, which means it is moderately related to PC1, but we cannot make any conclusion with respect to the other variables. We could say that values extremely close to 0, meaning that are not very correlated with PC1, probably are not related with credit_score, but its hard to say for certain. PC2 gives us much more information. The correlation for credit_score in PC2 is 0.29, and we can see other variables that are relatively large positive numbers and small negative numbers. We can definitely interpret this principle component. Looking at the large positive correlations, we can see that tradelines_avg_days_since_opened, max_cc_limit, and total_mortgage_loans_balance are strongly positively correlated with credit_score. This means that as these values increase, we would expect to see credit_score increase as well. As for the negative correlation coefficients, we can say that count_total_tradelines_opened_24_months, count_tradelines_condition_derogatory, and count_open_installment_accounts_24_months have an inverse relationship with credit_score. The other negative but lower magnitude coefficients could also have some sort of relationship with credit_score. We can continue down the line with PC3 and PC4, but the interpretability goes down as you analyze more principle components and they become less significant. 

From our PCA, we determined which features are correlated with each other, which provided us with useful information that was used for future analysis. Although the interpretability of PCA is lower than other models, we can still determine relationships between variables and datapoints. 


* 4.2 Applications for Regression

* 4.3 Applications for $k$-means Clustering: From our preprocessed data of clicks on different parts of the website, we start to run the k-means clustering algorithm. First, we need to decide on the number of clusters that we want. To do so, we run k-means for 1 to 20 clusters, calculating the within-sum-of-squares error for each. The within-sum-of-squares error tells us how separated our data is from the mean of the cluster. To have the best closest clusters, we want to minimize this value. 
```{r, echo=FALSE}
k.max <- 20
data <- user_engage_actions
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})

#ploting the wws vs. number of clusters. 
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```
The above graphic tells us that between 5 and 8 number of clusters seems to be the amount where the within-cluster sum of squares difference starts to drop off. Therefore, we will use 8 clusters, which will help with accounting for outliers. We run our k-means clustering on the dataset with k=8. 

```{r, echo=FALSE}
set.seed(10)
kmeanclusters = kmeans(user_engage_actions, 8, nstart=20)
kmeanclusters$centers
```
Looking at these cluster centers, we can see that cluster 7 and 4 have relatively high values for click_count_credit_card and click_count_personal_loan, respectively. This means that the clustered groups tend to click on the these sites more often. The other clusters are all more centered to the origin. This could have to do with the sparse natural of the dataset, which might be having an adverse effect on the results. Nonetheless, we can see two clearly different clustered groups in cluster 7 and 4. We then compare the user demographic for users of different groups. 
```{r}
#from the clustering, we can see that there are two clusters with larger values in particular columns (see report for interpretation). We want the user IDs of the people who had the inflated column values. 
click_credit_card = user_engage_data[which(kmeanclusters$cluster == 7),]
click_personal_loan = user_engage_data[which(kmeanclusters$cluster == 4),]

#creating a subset of user demographics that relate to the people with inflated column values
click_credit_users = subset(data.pre, data.pre$user_id %in% click_credit_card$user_id)
click_loan_users = subset(data.pre, data.pre$user_id %in% click_personal_loan$user_id)

#comparing the user demographics of the two groups of people
options(scipen=999)
#add this to the appendix
'''compare = matrix(c(mean(click_credit_users$is_homeowner), mean(click_loan_users$is_homeowner),
                   mean(click_credit_users$gender), mean(click_loan_users$gender),
                   mean(click_credit_users$tradelines_avg_days_since_opened), mean(click_loan_users$tradelines_avg_days_since_opened),
                   mean(click_credit_users$tradelines_max_days_since_opened), mean(click_loan_users$tradelines_max_days_since_opened),
                   mean(click_credit_users$tradelines_min_days_since_opened), mean(click_loan_users$tradelines_min_days_since_opened),
                   mean(click_credit_users$count_tradelines_closed_accounts), mean(click_loan_users$count_tradelines_closed_accounts),
                   mean(click_credit_users$count_total_tradelines_opened_24_months), mean(click_loan_users$count_total_tradelines_opened_24_months),
                   mean(click_credit_users$count_tradelines_cc_opened_24_months), mean(click_loan_users$count_tradelines_cc_opened_24_months),
                   mean(click_credit_users$count_tradelines_condition_derogatory), mean(click_loan_users$count_tradelines_condition_derogatory),
                   mean(click_credit_users$count_open_installment_accounts_24_months), mean(click_loan_users$count_open_installment_accounts_24_months),
                   mean(click_credit_users$count_tradelines_open_collection_accounts), mean(click_loan_users$count_tradelines_open_collection_accounts),
                   mean(click_credit_users$count_tradelines_open_mortgages), mean(click_loan_users$count_tradelines_open_mortgages),
                   mean(click_credit_users$count_tradelines_open_student_loans), mean(click_loan_users$count_tradelines_open_student_loans),
                   mean(click_credit_users$count_tradelines_opened_accounts), mean(click_loan_users$count_tradelines_opened_accounts),
                   mean(click_credit_users$count_tradelines_open_secured_loans), mean(click_loan_users$count_tradelines_open_secured_loans),
                   mean(click_credit_users$count_tradelines_open_unsecured_loans), mean(click_loan_users$count_tradelines_open_unsecured_loans),
                   mean(click_credit_users$total_tradelines_amount_past_due), mean(click_loan_users$total_tradelines_amount_past_due),
                   mean(click_credit_users$total_open_cc_amount_past_due), mean(click_loan_users$total_open_cc_amount_past_due),
                   mean(click_credit_users$total_cc_open_balance), mean(click_loan_users$total_cc_open_balance),
                   mean(click_credit_users$total_tradelines_open_balance), mean(click_loan_users$total_tradelines_open_balance),
                   mean(click_credit_users$max_cc_limit), mean(click_loan_users$max_cc_limit),
                   mean(click_credit_users$total_mortgage_loans_amount), mean(click_loan_users$total_mortgage_loans_amount),
                   mean(click_credit_users$total_mortgage_loans_balance), mean(click_loan_users$total_mortgage_loans_balance),
                   mean(click_credit_users$total_auto_loans_balance), mean(click_loan_users$total_auto_loans_balance),
                   mean(click_credit_users$total_student_loans_balance), mean(click_loan_users$total_student_loans_balance),
                   mean(click_credit_users$count_inquiries_3_months), mean(click_loan_users$count_inquiries_3_months),
                   mean(click_credit_users$count_inquiries_6_months), mean(click_loan_users$count_inquiries_6_months),
                   mean(click_credit_users$count_inquiries_12_months), mean(click_loan_users$count_inquiries_12_months),
                   mean(click_credit_users$count_bankruptcy), mean(click_loan_users$count_bankruptcy),
                   mean(click_credit_users$age), mean(click_loan_users$age),
                   mean(click_credit_users$credit_score), mean(click_loan_users$credit_score)),ncol = 2, byrow = TRUE)'''


compare = matrix(c(
                   mean(click_credit_users$count_tradelines_condition_derogatory), mean(click_loan_users$count_tradelines_condition_derogatory),
                   mean(click_credit_users$count_tradelines_open_collection_accounts), mean(click_loan_users$count_tradelines_open_collection_accounts),
                   mean(click_credit_users$total_tradelines_amount_past_due), mean(click_loan_users$total_tradelines_amount_past_due),
                   mean(click_credit_users$total_open_cc_amount_past_due), mean(click_loan_users$total_open_cc_amount_past_due),
                   mean(click_credit_users$total_cc_open_balance), mean(click_loan_users$total_cc_open_balance),
                   mean(click_credit_users$total_tradelines_open_balance), mean(click_loan_users$total_tradelines_open_balance),
                   mean(click_credit_users$max_cc_limit), mean(click_loan_users$max_cc_limit),
                   mean(click_credit_users$total_mortgage_loans_amount), mean(click_loan_users$total_mortgage_loans_amount),
                   mean(click_credit_users$total_mortgage_loans_balance), mean(click_loan_users$total_mortgage_loans_balance),
                   mean(click_credit_users$total_auto_loans_balance), mean(click_loan_users$total_auto_loans_balance),
                   mean(click_credit_users$credit_score), mean(click_loan_users$credit_score)),ncol = 2, byrow = TRUE)

colnames(compare) = c("click_credit", "click_loan")
rownames(compare) = names(click_credit_users[-c(1,2,3,4,5,6,7,8,9,11,13,14,15,16,17,26,27,28,29,30,31)])
compare
```
The above table is a subset of all of the features that differ between the two groups (check appendix for the full list of features). We can see that in several categories there is quite a large difference between the mean variables of the two clustered groups, and they are make sense. The users who click on the credit pages more often have a lower credit score (they click on the credit pages because they are interested in improving their credit), have inflated values that are associated with good credit (click_credit has lower cc_open_balance, max_cc_limit,count_tradelines_condition_derogatory). The click_loan group tend to have higher loan amounts, and they click on the loan pages more often possibly to learn how to best pay back their loans, or perhaps they clicked on the pages to take a loan. 

From the k-means clusters, we were able to determine that the demographics of people who use Credit Sesame's website differently and access different pages are in fact different. Naturally, the people who click more on credit or loans are the people who need help in improving those areas. By using k-means clustering, we were able to look at different people as a group, which meant we didn't have to compare individuals to other individuals, and it gives us a better view of what different groups of people are using Credit Sesame.

# 5 Discussion

